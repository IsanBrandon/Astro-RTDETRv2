"""Copyright(c) 2023 lyuwenyu. All Rights Reserved.
"""

import time 
import json
import datetime

import torch 

from ..misc import dist_utils, profiler_utils

from ._solver import BaseSolver
from .det_engine import train_one_epoch, evaluate


class DetSolver(BaseSolver):
    
    def fit(self, ):
        print("Start training")
        self.train()  # ì´ ë©”ì„œë“œì—ì„œ _solver.pyì˜ train ë©”ì„œë“œê°€ í˜¸ì¶œë˜ì§€ë§Œ,
                      # ì´ì œ í•´ë‹¹ ë©”ì„œë“œì—ëŠ” ë°ì´í„°ë¡œë” ì´ˆê¸°í™” ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.
        args = self.cfg

        n_parameters = sum([p.numel() for p in self.model.parameters() if p.requires_grad])
        print(f'number of trainable parameters: {n_parameters}')
        
        # ì´ì œ fit ë©”ì„œë“œì—ì„œ ë‘ ë°ì´í„°ë¡œë”ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        # YAML ì„¤ì •ì—ì„œ ì§ì ‘ shuffle ê°’ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
        self.train_dataloader_dsg = dist_utils.warp_loader(self.cfg.train_dataloader, 
                                                            shuffle=self.cfg.yaml_cfg['train_dataloader'].get('shuffle', True))

        self.train_dataloader_dds = dist_utils.warp_loader(self.cfg.train_dataloader_dds,
                                                            shuffle=self.cfg.yaml_cfg['train_dataloader_dds'].get('shuffle', True))

        self.val_dataloader = dist_utils.warp_loader(self.cfg.val_dataloader,
                                                      shuffle=self.cfg.yaml_cfg['val_dataloader'].get('shuffle', False))
        
        # ðŸ‘‡ 2. Mosaic ë³€í™˜ì— dataset ê°ì²´ë¥¼ ì „ë‹¬í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ðŸ‘‡
        # Dsg ë°ì´í„°ì…‹
        transforms_dsg = self.train_dataloader_dsg.dataset.transforms
        if hasattr(transforms_dsg, 'ops') and transforms_dsg.ops[0].__class__.__name__ == 'Mosaic':
            transforms_dsg.ops[0].dataset = self.train_dataloader_dsg.dataset

        # Dds ë°ì´í„°ì…‹
        transforms_dds = self.train_dataloader_dds.dataset.transforms
        if hasattr(transforms_dds, 'ops') and transforms_dds.ops[0].__class__.__name__ == 'Mosaic':
            transforms_dds.ops[0].dataset = self.train_dataloader_dds.dataset

        best_stat = {'epoch': -1, }
        start_time = time.time()
        start_epcoch = self.last_epoch + 1
        
        for epoch in range(start_epcoch, args.epoches):
            # êµëŒ€ í›ˆë ¨ ë¡œì§
            if epoch % 2 == 0:
                # ì§ìˆ˜ ì—í¬í¬: Dsg ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ (star vs galaxy)
                current_dataloader = self.train_dataloader_dsg
                is_dsg_epoch = True
            else:
                # í™€ìˆ˜ ì—í¬í¬: Dds ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ (smooth vs disk)
                current_dataloader = self.train_dataloader_dds
                is_dsg_epoch = False

            if dist_utils.is_dist_available_and_initialized():
                current_dataloader.sampler.set_epoch(epoch)
            
            train_stats = train_one_epoch(
                self.model, 
                self.criterion, 
                current_dataloader,  # ì„ íƒëœ ë°ì´í„°ë¡œë”ë¥¼ ì‚¬ìš©
                self.optimizer, 
                self.device, 
                epoch, 
                is_dsg_epoch=is_dsg_epoch, # is_dsg_epoch í”Œëž˜ê·¸ ì „ë‹¬
                max_norm=args.clip_max_norm, 
                print_freq=args.print_freq, 
                ema=self.ema, 
                scaler=self.scaler, 
                lr_warmup_scheduler=self.lr_warmup_scheduler,
                writer=self.writer
            )

            if self.lr_warmup_scheduler is None or self.lr_warmup_scheduler.finished():
                self.lr_scheduler.step()
            
            self.last_epoch += 1

            if self.output_dir:
                checkpoint_paths = [self.output_dir / 'last.pth']
                # extra checkpoint before LR drop and every 100 epochs
                if (epoch + 1) % args.checkpoint_freq == 0:
                    checkpoint_paths.append(self.output_dir / f'checkpoint{epoch:04}.pth')
                for checkpoint_path in checkpoint_paths:
                    dist_utils.save_on_master(self.state_dict(), checkpoint_path)

            # í‰ê°€ ì‹œì—ëŠ” Dsg ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
            module = self.ema.module if self.ema else self.model
            test_stats, coco_evaluator = evaluate(
                module, 
                self.criterion, 
                self.postprocessor, 
                self.val_dataloader,    # Dsg ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€
                self.evaluator, 
                self.device
            )

            # TODO 
            for k in test_stats:
                if self.writer and dist_utils.is_main_process():
                    for i, v in enumerate(test_stats[k]):
                        self.writer.add_scalar(f'Test/{k}_{i}'.format(k), v, epoch)
            
                if k in best_stat:
                    best_stat['epoch'] = epoch if test_stats[k][0] > best_stat[k] else best_stat['epoch']
                    best_stat[k] = max(best_stat[k], test_stats[k][0])
                else:
                    best_stat['epoch'] = epoch
                    best_stat[k] = test_stats[k][0]

                if best_stat['epoch'] == epoch and self.output_dir:
                    dist_utils.save_on_master(self.state_dict(), self.output_dir / 'best.pth')

            print(f'best_stat: {best_stat}')

            log_stats = {
                **{f'train_{k}': v for k, v in train_stats.items()},
                **{f'test_{k}': v for k, v in test_stats.items()},
                'epoch': epoch,
                'n_parameters': n_parameters
            }

            if self.output_dir and dist_utils.is_main_process():
                with (self.output_dir / "log.txt").open("a") as f:
                    f.write(json.dumps(log_stats) + "\n")

                # for evaluation logs
                if coco_evaluator is not None:
                    (self.output_dir / 'eval').mkdir(exist_ok=True)
                    if "bbox" in coco_evaluator.coco_eval:
                        filenames = ['latest.pth']
                        if epoch % 50 == 0:
                            filenames.append(f'{epoch:03}.pth')
                        for name in filenames:
                            torch.save(coco_evaluator.coco_eval["bbox"].eval,
                                    self.output_dir / "eval" / name)

        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print('Training time {}'.format(total_time_str))


    def val(self, ):
        self.eval()
        
        module = self.ema.module if self.ema else self.model
        test_stats, coco_evaluator = evaluate(module, self.criterion, self.postprocessor,
                self.val_dataloader, self.evaluator, self.device)
                
        if self.output_dir:
            dist_utils.save_on_master(coco_evaluator.coco_eval["bbox"].eval, self.output_dir / "eval.pth")
        
        return
